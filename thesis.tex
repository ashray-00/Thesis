\documentclass[a4paper,11pt,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{helvet}
\usepackage[english]{babel}     %% typographie française
\usepackage[style=numeric,language=english,backend=bibtex]{biblatex}
\usepackage{parskip}		%% blank lines between paragraphs, no indent
\usepackage[margin=1cm]{caption}%% give long captions a margin
\usepackage{booktabs}           %% typesetting nice tables
\usepackage{subfig}
\usepackage[pdftex]{graphicx}	%% include graphics, preferrably pdf
\usepackage[pdftex]{hyperref}	%% many PDF options can be set here
\usepackage{algorithm}
\usepackage{arevmath}     % For math symbols
\usepackage{amsmath}
\usepackage[noend]{algpseudocode}
\pdfadjustspacing=1		%% force LaTeX-like character spacing

\newcommand{\myname}{Ashray Adhikari}
\newcommand{\mytitle}{Underwater Image Enhancements}
\newcommand{\mysupervisor}{Prof. Andreas Birk}

\hypersetup{
  pdfauthor = {\myname},
  pdftitle = {\mytitle},
  pdfkeywords = {},
  colorlinks = {true},
  linkcolor = {blue}
}

\addbibresource{references.bib}

\begin{document}
  \pagenumbering{roman}

  \thispagestyle{empty}

  \begin{flushright}
    \includegraphics[scale=0.8]{bsc-logo}
  \end{flushright}
  \vspace*{40mm}
  \begin{center}
    \huge
    \textbf{\mytitle}
  \end{center}
  \vspace*{4mm}
  \begin{center}
   \Large by
  \end{center}
  \vspace*{4mm}
  \begin{center}
    \LARGE
    \textbf{\myname}
  \end{center}
  \vspace*{20mm}
  \begin{center}
    \Large
    Bachelor Thesis in Computer Science
  \end{center}
  \vfill
  \begin{flushleft}
    \large
    Submission: \today \hfill Supervisor: \mysupervisor \\
    \rule{\textwidth}{1pt}
  \end{flushleft}
  \begin{center}
    Jacobs University Bremen $|$ Department of Computer Science and Electrical Engineering
  \end{center}

  \newpage
  \thispagestyle{empty}

\subsection*{English: Declaration of Authorship}
 
  I hereby declare that the thesis submitted was created and written
  solely by myself without any external support. Any sources, direct
  or indirect, are marked as such. I am aware of the fact that the
  contents of the thesis in digital form may be revised with regard to
  usage of unauthorized aid as well as whether the whole or parts of
  it may be identified as plagiarism. I do agree my work to be entered
  into a database for it to be compared with existing sources, where
  it will remain in order to enable further comparisons with future
  theses. This does not grant any rights of reproduction and usage,
  however.

  This document was neither presented to any other examination board
  nor has it been published.

  \subsection*{German: Erklärung der Autorenschaft (Urheberschaft)}
 
  Ich erkläre hiermit, dass die vorliegende Arbeit ohne fremde Hilfe
  ausschließlich von mir erstellt und geschrieben worden ist. Jedwede
  verwendeten Quellen, direkter oder indirekter Art, sind als solche
  kenntlich gemacht worden. Mir ist die Tatsache bewusst, dass der
  Inhalt der Thesis in digitaler Form geprüft werden kann im Hinblick
  darauf, ob es sich ganz oder in Teilen um ein Plagiat handelt. Ich
  bin damit einverstanden, dass meine Arbeit in einer Datenbank
  eingegeben werden kann, um mit bereits bestehenden Quellen
  verglichen zu werden und dort auch verbleibt, um mit zukünftigen
  Arbeiten verglichen werden zu können. Dies berechtigt jedoch nicht
  zur Verwendung oder Vervielfältigung.

  Diese Arbeit wurde noch keiner anderen Prüfungsbehörde vorgelegt
  noch wurde sie bisher veröffentlicht.

  \vspace{20mm}

  Date, Signature

  \newpage
  \tableofcontents

  \clearpage
  \pagenumbering{arabic}

  \section{Introduction}

  %This, like the rest, addresses fellow experts from your field (but
  %not from your particular topic of research). Here you should
  %technically connect to the main concepts from that field and give an
  %outline of your project, stating the research/engineering question
  %that you want to get answered by your project.

  %(target size: 1-2 pages)
  
  Researchers today strive to capture high-quality underwater images for various Underwater activities \cite{15} but, Underwater imaging is a challenging field in computer vision research instead of land photography. Many physical and chemical characteristics of Underwater conditions raise issues that we can easily overcome in land photography \cite{15}.\\
  \\
  Underwater imaging raises new challenges and problems due to the absorption and scattering effects of light underwater, which generates low-contrast images and causes distant objects to appear blurry and misty \cite{3}. As light moves from air to water, it is partly reflected, and the amount of light entering reduces as we start going deeper. So, Underwater images appear dark as depth increases and the colors drop off one by one depending on the wavelength \cite{4}. Due to the varying degrees of attenuation encountered by the light traveling in water with different wavelengths, the ambient underwater environment is dominated by a bluish tone \cite{20}.\\
  The light received by the camera is mainly generated by a direct component that reflects light from the object, forward scattering that randomly deviates light on its way to the camera, and the backscattering component that reflects light toward the camera before the light reaches the objects \cite{21}. In an underwater scene, light accepted through a camera can be represented as a linear superposition of all three components.  So, forward scattering causes the blurring on the image, and the backscattering masks the details of the scene \cite{21}.\\
  \\
  Besides the absorption and scattering effects of light by the water, water quality also influences the quality of underwater images. The presence of suspended particles with significant size and density in the medium also causes light to be scattered and absorbed \cite{8}. Haze is caused by the suspended particles such as sand, minerals, and plankton in the water body. Some portion of the light meets these suspended particles as light reflected from the object propagates towards the camera \cite{13}.\\
  \\
  Due to certain challenging underwater conditions, traditional enhancing techniques appear to be strongly limited for Underwater Images. Traditional image restoration methods use an atmospheric scattering model to restore underwater images\cite{11}, and underwater conditions are considered similar to dense fog on land. Still, unlike fog, underwater illumination is spectrally deprived as water attenuates different wavelengths of light to different degrees \cite{2}. Applying conventional color correction methods designed for terrestrial environments on Underwater images will lead to undesired artifacts since the red component of the Underwater image is much weaker than the green and blue components \cite{6}.\\
  \\
  In this document, we test the effects and analyze quantitatively and qualitatively the result of different machine vision methods for Underwater Image Enhancement on images extracted from Underwater videos of the Valentine memorial. The rest of the document is structured as follows. Section \hyperref[sec:2]{2} discusses the related work done in Underwater Image Enhancement. In section \hyperref[sec:3]{3} some Underwater Enhancement methods are discussed in-depth, and the methods to test and analyze the extracted images are defined. Section \hyperref[sec:4]{4} showcases the results and analysis of the experiment on the extracted images. Section \hyperref[sec:5]{5} provides the conclusion of the document.  

  \section{Discussion of Related Work}
  \label{sec:2}

  %This part should make clear which question, exactly, you are
  %pursuing, and why your project is relevant/interesting. This is the
  %place to explain the background and to review the existing
  %literature. Where does your project extend the state of the art?
  %What weaknesses in known approaches do you hope to overcome? If you
  %have carried out preliminary experiments, describe them here.

  %(target size: 5-10 pages)
  Over the years, several methods have been specifically designed according to the characteristics of underwater images, e.g., hazing, color casts, and low contrast \cite{15}. This section will discuss different physics-based, color correction, haze removal, fusion-based, and CNN-based Underwater Image Enhancements and Restoration methods.\\
  \\
  In a Physics-based approach for recovery of visibility in Underwater scenes, raw images are taken through different stages of a polarizing filter. In 2004 Schechner and Kerpel \cite{19} proposed a physics-based approach that inverts the image formation process to recover a good visibility image of the object. The algorithm presented were based on photos taken through a polarizer at different orientations. The physics-based methods require high computing resources and long execution time, and the polarizer used in the approach is expensive \cite{8}. We will not consider the physics-based method for our analysis.\\
  \\
  In recent years many studies have proved the effectiveness of deep-learning methods for various computer vision tasks. But, very few of them have been effective in underwater image enhancement. In 2017, Perez et al. \cite{22} proposed a CNN-based Underwater Image Enhancement method that uses a convolution neural network to learn the transformation from raw acquired images to enhanced images using a pair of degraded and recovered underwater photos \cite{15}. Then in 2018, Li et al. \cite{23} proposed WaterGAN, a generative adversarial network for generating synthetic realistic underwater images from in-air image and depth maps, where the raw Underwater, actual color-in air, and the depth data were used to feed a deep learning network for correcting color casts in underwater images \cite{15}. To relax the need for paired underwater images for network training and allow the use of unknown underwater images, Li et al. \cite{7} proposed an "Underwater Image Correction model based on a Weakly Supervised Color Transfer" \cite{15}. In this method, a color correction algorithm is applied to correct the color cast and produce a natural appearance of the sub-sea image. Then a pair of dictionaries based on sparse representation are used to sharpen the image and enhance the details \cite{7}. Even though the Deep Learning-based Underwater Enhancement methods have, to some extent, proven to be effective in enhancing underwater images, we will not use the CNN-based model to enhance the extracted images from the Valentine memorial due to the lack of data required to train the model.\\
  \\
  Most enhancement methods are scene-dependent and need prior knowledge. Farhadifard et al. \cite{14} in 2015 proposed an image enhancement method that "combines learning-based de-blurring and adaptive color correction." First, the degraded image is white balanced using an adaptive gain factor to compensate color cast, which produces a natural appearance of the underwater medium. Then, the partially restored version is enhanced by a learning-based de-blurring algorithm \cite{14}. Two global dictionaries are trained based on sparse representation—one from a clear and high-quality underwater image set and the second one using the corresponding blurred version. Blur free and sharpened image is obtained based on basic principle between the high and low patches \cite{14}.\\
  \\
    Classic Retinex theory assumes that the images can be decomposed into two components, reflectance, and illumination. The classic Retinex model is not suited for the low-light image enhancement problem, as intensive noise exists in low-light images \cite{5}. In 2018 Li et al. \cite{5} proposed a "Structural Revealing Image Enhancement Algorithm Using a Robust Retinex Model." This method proposed a Robust Retinex, which additionally considers a noise map compared to the conventional Retinex model to improve the performance of enhancing low-light images accompanied by intense noise \cite{5}. Even though this method was not made for underwater conditions, it can be generalized to handle Underwater Image Enhancements \cite{5}.\\
  In 2019 B. Gao et al. \cite{6} proposed an "Underwater Image Enhancement model inspired by the morphology and function of the teleost fish retina to solve the blurring and the non-uniform color biasing problem of underwater images." The motivation behind this approach is that the visual system of the oceanic creatures has evolved to adapt to the natural statistics of aquatic scenes \cite{6}. So, modeling the perception and imaging mechanisms of the oceanic creatures will give us new insights into processing Underwater images \cite{6}.\\
  \\
   Wavelet decomposition algorithm decomposes an original image into a sequence of new images of decreasing size \cite{24}. In a fusion-based approach, weight maps are computed and fused to enhance the image \cite{10}.  In 2016 A. Khan et al. \cite{9} proposed a "wavelet-based fusion method to enhance hazy underwater images by addressing low contrast and color attenuation issues." Two input images are generated from the hazy image by color correction using histogram stretching, and contrast enhancement using contrast limited adaptive histogram equalization (CLAHE) \cite{9}. Both input images are decomposed into wavelet coefficients, and the coefficient is fused using an algorithm that eliminates unwanted low and high frequency present in the image. Finally, the inverse composition is applied to the fused coefficient to get a synthesized enhanced image. \cite{9}\\
  In 2017 Wang et al. \cite{12} proposed a different "fusion-based model applied to a frequency domain." It involves two inputs represented as color corrected and contrast-enhanced images extracted from the original image. The input images are decomposed into low frequency and high-frequency components using a three-scale wavelet operator. Then, Multi-scale fusion principles are applied to the low and high-frequency components. The fusion principle for low-frequency components involves a weighted average of the components, and the high-frequency component involves the local variance of components. The fused image is obtained by reconstructing the new fused low frequency, and high-frequency components \cite{12}.\\
  \\
  As the weight maps computed in a fusion-based approach may introduce artifacts in the image, P. Honnutagi et al. in 2018 \cite{10} proposed a method to overcome this problem by using an "optimized fusion method of weight maps, where multi-step fusion is used to perform a fusion of weight maps computed from illumination estimation and contrast-enhanced inputs of the original image." First illumination estimation is applied on the image to improve visibility, then contrast limited adaptive histogram equalization (CLAHE) is used to derive the second input, which improves the contrast of the image \cite{10}. Saliency, luminance, and chromatic weight maps are computed for each derived inputs. After computing each weight map, the final weight map is produced by multiplying all three weight maps. Finally, an enhanced image is generated from multi-step fusion technique by using the Gaussian and mean operators on both weight maps \cite{10}.\\
  \\
  The Dark Channel Prior method is based on the observation that in a clear day image, there exists some pixels which have a very low intensity in at least one color channel \cite{15}. Due to the similarity between hazed outdoor image and underwater image Dark Channel-based hazing method is used in underwater image enhancement. Chao and Wang in 2010 \cite{25} used the "Dark Channel Prior method directly to enhance the clarity of the image." Using dark channel prior depth of the Underwater Image is estimated, but this method does not solve the problem of absorption \cite{25}.\\
 In 2011 Yang et al. \cite{8} proposed a "DCP-based fast Underwater Image Enhancement method that can effectively enhance Underwater Image and reduce execution time." This method used a median filter instead of a soft matting procedure to estimate the depth map of the image. The Dark Channel Prior is calculated to estimate air-light, and the depth map is generated using a median filter. A color correction method is employed to enhance the color contrast of the image.\cite{8}\\
  \\
  As enormous scene depth causes more object blurriness in Underwater images, Y. Peng et al. in 2015 \cite{3} proposed a method that "measures scene depth from image blurriness to estimate depth map for Underwater Image enhancement." The method is based on the observation that objects farther from the camera are more blurry for Underwater Images. To overcome the limitation of DCP, the depth estimation method calculates the difference between the original and the multi-scale Gaussian filter to estimate the pixel blurriness map. Then, by assuming that the depth in the small local patch is uniform, the max filter is applied to the pixel blurriness map. Finally, closing by morphological reconstruction (CMR) and the guided filter are used to refine the depth map \cite{3}.\\
   Most existing methods based on the DCP method convert information from three RGB channels to one dark channel. Still, those methods are inadequate in cases where chromatic information is essential for restoration \cite{17}. Zhou et al. in 2019 \cite{17} proposed a "method to handle the scattering and absorption problems of light with different wavelengths base on the color-line model." The color line is a regularity in images, which shows that the pixel values of small patches are distributed along a  one-dimensional line in the RGB space \cite{17}. First, the image patches that exhibit the characteristics of the color-line prior are filtered out, and the color line of the patches are recovered \cite{17}. "Then, the local transmission for each patch is estimated based on the offsets of the color lines along the background-light vector from the origin" \cite{17}. An alternating iterative algorithm is also presented here to solve the non-linear optimization problem for transmission in an Underwater environment \cite{17}. Finally, a wavelength-compensation method is also proposed to correct the color cast caused by water depth \cite{17}.\\
  \\
  The transmission map is reducing exponentially with scene depth, which is difficult to estimate in Underwater Environment. So, Mi et al. in 2018 \cite{11} proposed an "enhancement method using a multi-scale gradient-domain contrast enhancement strategy to increase visibility and compensate the attenuation of color saturation according to the estimated transmission." Before applying the enhancement method, first white balance the image to remove the color casts while producing a natural appearance of the Underwater image \cite{11}. Then, a multi-scale gradient enhancement strategy is used to prevent halo artifacts. Gradient enhancement is processed on multiple residual images produced by edge-decomposition \cite{11}. By compensating color saturation according to the estimated transmission, we get the enhanced image \cite{11}.\\ 
  \\
  Several other methods have been proposed to remove the haze and increase the contrast of Underwater images. Ancutti et al. in 2018 \cite{16} proposed a method that uses "color balance and fusion methods to enhance Underwater Images." The White Balanced version of the original image is decomposed into two input images to enhance the color contrast and edge sharpness. Then, the weight maps are defined to preserve the qualities and reject the defaults of the inputs \cite{16}. Finally, the multi-scale fusion method decomposes the image into a sum of bandpass images. The method is motivated by the human visual system, which is very sensitive to sharp transition appearing in smooth image \cite{16}.\\
  \\
  Since most Underwater dehazing methods are for still underwater images, Emberton et al. \cite{2} in 2018 proposed a "dehazing method that improves visibility in images and videos by detecting and segmenting image regions; that contain only water." The spectral distortion problem is addressed in the method by automatically selecting the most appropriate white balancing method based on the most dominant color of the water \cite{2}. Varying patch sizes are used to estimate light that is scattered from underwater particles into the line of sight of the camera \cite{2}. Entropy-based segmentation is used to localize pure haze region that informs the features to select for the scattered light \cite{2}. Image-specific transmission value is allocated to pure haze regions to avoid generating artifacts \cite{2}.\\
  \\
   The presence of an artificial light source in Underwater Image increases the scattering of light Underwater. So, Chiang and Chen in 2012 \cite{13} proposed a method to "enhance underwater images by dehazing algorithm to compensate the attenuation difference along the propagation path, and to take the influence of the possible presence of an artificial light source into consideration." Dark channel prior is first used to estimate the distances of scene objects to the camera. Based on the depth map, the foreground and background areas within the image are segmented. The light intensity of the foreground and background are compared to determine whether artificial is used. Dehazing algorithm and wavelength compensation are used to remove the haze effect, and color change along the Underwater propagation path to the camera \cite{13}.\\
  \\
  Histogram equalization involves transferring the histogram of an original image to an evenly distributed form \cite{1}. For an Underwater image histogram equalization method is mainly used for color correction of the degraded image. Beilei Hu et al. \cite{1} in 2013 proposed a method that uses "inhomogeneous illumination to get a clearer image, and then processes it to use color image histogram equalization to get more color information which gives out true color correction." Inhomogeneous intensity can compensate for the loss of light intensity due to light attenuation and decrease backscattering effect caused by the scattering of Underwater light \cite{1}. As the light in Underwater is heavily attenuated, if the light source is not strong enough, the intensity of the target object in the image is very weak. So, it is necessary to use inhomogeneous illumination as the light source \cite{1}. During color histogram equalization, each channel of the image is histogram equalized by monotonically increasing grey level transform. These images of different color channels combined to form a high contrast image \cite{1}.\\
  \\
   Underwater Images are used for many different computer vision applications, and different kinds of images are required. So, Li et al. in 2016 \cite{21} proposed an "Underwater Enhancement algorithm, which includes both image dehazing and contrast enhancement algorithm that is built on minimum information loss principle to restore the visibility, color, and natural appearance of Underwater images." Two different output images are produced for applications using this method. The first version with relatively natural color and natural appearance is suitable for displaying the image, and the second version with high contrast and brightness is used for extracting more valuable information and revealing more details \cite{21}. For the image dehazing algorithm, the global background light is first estimated using a hierarchical searching technique based on quad-tree subdivision followed by removing suspended particles using the dark channel prior algorithm. The disturbance of bright objects is removed, and the global background light is determined according to the properties of light traveling in water \cite{21}. Then, to minimize information loss, the medium transmission map of the color channels is defined. Underwater images with high contrast and brightness play a vital role in object detection, classification, etc. \cite{21}. The contrast enhancement algorithm proposed in this method is based on the histogram distribution of visually appealing natural-scene images \cite{21}. The histogram distribution prior is counted for five natural-scene image datasets. Then, the average histogram distributions of natural scenes are regarded as a template and are followed by the approximate histogram matching to adjust the histogram distributions of haze-free underwater images obtained by the dehazing algorithm \cite{21}.\\
  \\
  Most Underwater images are degraded by light absorption and scattering, which causes a single color to dominate the image. Iqbal et al. in 2007 \cite{4} proposed a method based on slide stretching. First, contrast stretching of the RGB algorithm is used to equalize color contrast in the image. Then, the RGB image is transformed into HSI color space, using the saturation and intensity transfer function to increase the actual color and brightness of Underwater images \cite{4}. Using two side stretching models helps equalize the color contrast in images and address lighting \cite{4}.
 \newpage
  \section{Methodology}
  \label{sec:3}
  %This is the technical core of the thesis. Here you lay out your how
  %you answered your research question, you specify your design of
  %experiments or simulations, point out difficulties that you
  %encountered, etc.
  %
  %(target size: 5-10 pages)
  In the previous section, we saw varieties of Underwater image enhancement methods that strive to solve different problems raised by Underwater imaging. While almost all those methods may solve various issues raised by Underwater imaging, we will not test and use all those methods due to the lack of data or information about our Underwater Scene. So, we have selected \cite{4}, \cite{8}, \cite{26} and \cite{16} methods to test and compare the result for our Underwater image extracted from the Underwater videos of the Valentine memorial. These methods help solve some of the common problems in Underwater imaging, which gives us information on which plan would be best suited for our Underwater images. Furthermore, we use the An Underwater Color Image Quality Evaluation Metric(UCIQE) \cite{18} for the quantitative comparison of the enhancement methods.\\
  \\
  If we look at our images, we can see that the quality of the Underwater image is affected by different Underwater image characteristics. The Underwater image has low color contrast and has a severe blurring effect. So, we use the selected Underwater enhancement methods to enhance and compare the output images.\\
  \begin{figure}[htp]
  \begin{center}
    \subfloat[]{\includegraphics[scale=0.3]{../Underwater-Image-Enhancements/InputImages/Valentine_196.png}}
    \subfloat[]{\includegraphics[scale=0.3]{../Underwater-Image-Enhancements/InputImages/Valentine_95.png}} \\
    \subfloat[]{\includegraphics[scale=0.3]{../Underwater-Image-Enhancements/InputImages/Valentine_129.png}}
    \subfloat[]{\includegraphics[scale=0.3]{../Underwater-Image-Enhancements/InputImages/Valentine_30.png}}
  \end{center}
  \caption{Original Degraded images}
  \label{fig:edge}
\end{figure}

  First, we need to learn about these Underwater enhancement methods and the problems these methods are trying to solve.
  \subsection{Underwater Image Enhancement using an Integrated Color Method \cite{4}}
  As discussed above Integrated Color Method is based on slide stretching. In this method, we use the HSI(Hue, Saturation, or Intensity) color model, as it provides a more comprehensive color range by controlling the color elements of the image \cite{4}. For instance, the 'S' and 'I' value controls the blue color element in the image to create a range from pale blue to deep blue. We can use this color model to control the contrast ratio in Underwater Images by increasing or decreasing the value \cite{4}.\\
We can express the Integrated Color Model approach using the following algorithm.
  \begin{algorithm}
	\caption*{Underwater Image Enhancement using an Integrated Color Method \cite{4}}
	\begin{algorithmic}

	\Procedure{Enhance}{degraded\_image}
    \State Contrast Stretching Of RGB Image
    \State Transform RGB Images to HSI \Comment{Using Saturation and Intensity transfer function}
    \State Stretch Saturation and Intensity of the HSI image  \Comment{Using the transform function}
    \State Transform Image Back to RGB color space
    \State
    
    \Return enhanced\_image
\EndProcedure

\end{algorithmic}
\end{algorithm}\\
We used a contrast stretching algorithm to enhance the contrast of the Underwater Image. When applying contrast stretching to color images, each color channel is stretched using the same scaling to maintain the correct color ratio. In a Contrast stretching algorithm, a linear scaling function to the pixel value is used. We scale each pixel in the Image using the following equation \cite{4}.
$$P_{o} = (P_{i} - l_c) \times (max_v - l_c)  / (h_c - l_c) + min_v$$
Where \cite{4},
\begin{itemize}
\item
$P_{0}$ is the normalized output pixel value
\item
$P_{i}$ is the input/considered pixel value
\item
$min_v$  minimum value of the output range and since for RGB we are using image value from 0 to 255, $min_v = 0$
\item
$max_v$ maximum value of the output range and since for RGB we are using image value from 0 to 255, $max_v = 255$
\item
$l_c$ is the lowest pixel value currently present in the image
\item
$h_c$ is the highest pixel value currently present in the image 
\end{itemize}
To balance the red and green channel to that of the blue channel first histogram stretching into both sides is performed to get a well-balanced histogram. Using the HSI color model, we can get the true color of the Underwater images using the saturation parameter. The lighting problem in Underwater scenes is solved using the Intensity parameters. \cite{4} \\
\\
This method attempts to solve the lighting problem and extract the underwater images' true colors but mostly fails to enhance the highly deteriorated Underwater images.

\subsection{Low Complexity Underwater Image Enhancement Based on Dark Channel Prior \cite{8}}
Dark Channel Prior (DCP) is a popular classic Image Enhancement method. Most dark channel prior methods use a soft matting algorithm to eliminate the transmission to reconstruct a scene, but this requires heavy computing resources to smooth and optimize the transmission map \cite{8}. So, in this method, we estimated transmission $\tilde{t}(x)$ directly using the median filter, then to improve the color contrast of the object, we use an unsupervised color correction method \cite{8}.\\
We can model the captured image as direct reflection of light from the object and the reflection of particles from the medium. So, the captured image I(x) is defines as \cite{8}:
$$I(x) = J(x)t(x) + A(1-t(x))$$
J(x) is the output scene radiance,t(x) is the transmission map of the medium, and A is the global atmospheric light. Here, the first term is the direct attenuation, and the second term is called the air-light \cite{8}. Then, we can use this equation for the output scene radiance J(x) by rewriting the equation as \cite{8}:
$$J(x) = \frac{(I(x) - A)}{t(x)} + A$$
For the above equation,  we computed the atmospheric light A using the Dark channel prior method. As the dark channel of the hazy image approximates the haze denseness, we can use the dark channel prior to detect the highest haze-opaque region. For, this method we use the top $0.1\%$ of the brightest pixels in the dark channel \cite{8}. Then, among those pixels, we select the pixels with the highest intensity in the observed image as the atmospheric light A \cite{8}.\\
We use the median filter to calculate the estimated transmission map of the input image for this method. We can express the estimated transmission $\tilde{t}(x)$ using the following equation.
$$\tilde{t}(x) = 1 - \min_{c} (\underset{y\in\psi(x)}{median} \frac{I_{c}(y)}{A_{c}})$$
After computing the scene radiance of the image we used the color correction algorithm to achieve a good visual quality image. In Underwater conditions, images are not color balanced correctly as blue color dominates most of the images \cite{8}. So, first we need to calculate the average of each color components \cite{8}. If we assume size of the image is $M \times N$ and the images $I_R, I_G, I_B$ represent the red, green and blue compoents of the image \cite{8}. We can calculate the average of each color channels as \cite{8}:
$$R_{avg} = \frac{\sum_{i=1}^{M} \sum_{j=1}^{N}I_{R}(i,j)}{M \times N}$$
$$G_{avg} = \frac{\sum_{i=1}^{M} \sum_{j=1}^{N}I_{G}(i,j)}{M \times N}$$
$$B_{avg} = \frac{\sum_{i=1}^{M} \sum_{j=1}^{N}I_{B}(i,j)}{M \times N}$$
Since, Underwater images is dominated mostly by blue color, we can use the blue color to balance the red and green colors. So, using the blue color channel, the other two colors are calculated as follows \cite{8}:
$$\tilde{I_R}(i,j) = \frac{B_{avg}}{R_{avg}}I_R(i,j)$$
$$\tilde{I_G}(i,j) = \frac{B_{avg}}{G_{avg}}I_G(i,j)$$
We can summerize the "Low Complexity Underwater Image Enhancement Based on Dark Channel Prior" \cite{8} method using the following algorithm.
\begin{algorithm}
	\caption*{Low Complexity Underwater Image Enhancement Based on Dark Channel Prior \cite{8}}
	\begin{algorithmic}

	\Procedure{Enhance}{degraded\_image}
    \State Calculate the Dark Channel
    \State Get Atmospheric Light A using the Dark Channel \Comment{With top 0.1\% pixel brightness}
    \State Calculate the Transmission Map $\tilde{t}$
    \State Calculate the output scene radiance $J(x)$
    \State
    \State Enhance the scene radiance using the Color Correction method
    \State
    
    \Return enhanced\_image
\EndProcedure

\end{algorithmic}
\end{algorithm}\\
This method uses the dark channel prior algorithm, and it may not perform well for heavily degraded images with higher light absorption and scattering. 
\subsection{Single underwater image restoration by blue-green channels dehazing and red channel correction \cite{26}}
The accuracy of assumptions, optimal model, and estimated parameter limits most of the Underwater dehazing methods. Based on the fact that red light is most easy to get absorbed than green and blue,  Li et al. \cite{26} proposed an Underwater Image enhancement method based on blue-green channel dehazing and red channel color correction.\\
In this method, we assume that the attenuation of the red light only results from the absorption of light and the attenuation of green and blue light only results from the scattering of light. We correct the green and blue color channels with a dehazing algorithm based on Dark Channel Prior(DCP) modifications. The red color channel is corrected using color correction using Gray-world assumption. \cite{26}\\
For this method, we can define the Underwater imaging model as \cite{26}:
\begin{equation}
\label{eq:eq1}
I^{c}(x) = J^{c}(x) t(x) + B^{c} (1 - t(x)), c \in \{g, b\}
\end{equation}
Where I(x) is the input image, J(x) is the output scene radiance, t(x) is the median transmission map of the image that represents the percentage of scene radiance that reaches the camera, and B is the background light of the image. In the dehazing process we the scene radiance(J(x)), transmission map(t(x)) and the background light(B) from the input image I(x). \cite{26}\\
\\
We can estimate Background light(B) because the red channel is absorbed faster than the green and blue channel. To determine the differences among the three colors, we compare the maximum intensities of the red channel to those of the green and blue channels. With D(x) denoting the largest difference between the three color channels and $\Omega$ denoting the local path of the image I, we can write D(x) as \cite{26}:
$$D(x) = \max_{x \in \Omega, c \in r}I^{c}(x) - \max_{x \in \Omega, c \in \{g,b\}}I^{c}(x)$$
From difference (D(x)) we can estimate Background light (B) as \cite{26}:
$$B^c = avg(I^c(arg \min_{r}D(x))), c \in \{g,b\}$$
Based on the Rayleigh scattering theory, the blue and green light distortion is the same in water. We can assume that the median transmission map of the blue and green channel is identical, and the median transmission map in a local patch $\Omega$ is constant \cite{26}. If we arrange Eq(\eqref{eq:eq1}) and find the minimum in local patch we get \cite{26}:
$$\min_{c}(\min_{x\in\Omega}(\frac{I^{c}(x)}{B^{c}}))=t(x)\min_{c}(\min_{x\in\Omega}(\frac{J^{c}(x)}{B^{c}})+1-t(x)$$
As scene radiance, $J^{c}$ is a haze-free image, according to \cite{27} dark channel of J is close to zero due to dark channel prior, and since $B^c$ is always positive, the first term on the right side should be zero. So, we can write the transmission map of the green and blue channel as \cite{26}:
$$t(x)=1- \min_{ c\in\{g,b\}}(\min_{x\in\Omega}(\frac{I^{c}(x)}{B^{c}}))$$
Since we calculated the transmission map t(x) over the image patch, which produces a rough initial estimate of the map, we get some halos and artifacts on the image. We used the guided filter to refine the coarse map and address the problem. \cite{26}\\
Finally, after calculating the transmission map and the background light of the green and blue channels of the image, the dehazed green and blue channels are represented as \cite{26}:
$$J^{c}(x)=\frac{I^{c}(x)-B^{c}}{t^{c}(x)}+B^{c}, c\in\{g,\ b\}$$
After solving the dehazing problem, we use the gray-world assumption theory to solve the absorption problem of the red color channel. Gray-world assumption theory states that the average value of the object color in an ideal image is gray \cite{26}. So, using the following assumption, we can write \cite{26}:
$$(avgRr+avgBr+avgGr)/3=0.5$$
$$avgRr=1.5-avgBr-avgGr,$$
where $avgRr$, $avgBr$ and $avgGr$ are the normalized average value of the recovered image in the red, blue, and green channel, and $0.5$ is the gray value an image.\\
We can find the enhanced red channel as \cite{26}:
$$R_{enh} = R. * \frac{avgRr}{avgR},$$
$R$ represents the normalized original red channel, and $avgR$ represents the normalized average value of the original red channel.\\
After the green-blue channel dehazing and color correction, dark or bright regions in the Underwater Image appear too dark or too bright. So, we calculate adaptive exposure map s(x) by solving the following optimization problem to adjust our results \cite{26}.
$$\min_{s}\sum_{x}\{[1-s(x)\frac{Y_{J(x)}}{Y_{I(x)}}]^{2}+\lambda[s(x)-1]^{2}\}+\Phi(s),$$
where, $Y_{J(x)}$ is the illumination intensity of the enhanced scene radiance, $Y_{I(x)}$ is the illumination intensity of input image, $\lambda$ is a constant which is set to 0.3 and $\Phi(.)$ is the smoothness regularization \cite{26}.\\
 To get a fast approximate solution, we can solve the adaptive exposure map s(x) first without the smoothness regularization and then apply guided filter $GF_I$ to smooth the solution \cite{26}.
 $$s(x)=GF_{I}[\frac{Y_{J(x)}Y_{I(x)}+\lambda Y_{I(x)}^{2}}{Y_{J(x)}^{2}+\lambda Y_{I(x)}^{2}}]$$
 Finally, we can find the final enhanced image as:
 $$output=J^{c}(x).\ *s(x), c\in\{r,\ g,\ b\}$$
 We can summarize the "Single underwater image restoration by blue-green channels dehazing and red channel correction" \cite{26} with the following algorithm.
 \begin{algorithm}
	\caption*{Single underwater image restoration by blue-green channels dehazing and red channel correction \cite{26}}
	\begin{algorithmic}

	\Procedure{Enhance}{degraded\_image}
    \State Find the largest difference between the color channels
    \State Estimate Background light 
    \State Generate the transmission map of green-blue channel
    \State Calculate refined transmission map using the guided filter
    \State Generate green-blue dehazed image
    \State
    \State Enhance red color channel using Gray-world assumption
    \State
    \State Calculate the Adaptive exposure map
    \State Generate the final Adaptive Scene Radiance
    \State
    
    \Return enhanced\_image
\EndProcedure

\end{algorithmic}
\end{algorithm}\\
\subsection{Color Balance and Fusion for Underwater Image Enhancement \cite{16}}
In this method, we use the multi-scale fusion method to fuse two input images. We derive the two input images from a color corrected and white-balanced version of the original image. We use the multi-scale fusion method to avoid artifacts in low-frequency components created by the sharp weight map transitions in the enhanced image.\cite{16}\\
Most white-balancing methods make assumptions to estimate the light source's color and find the color balance by dividing each color channel by corresponding normalized light source intensity.  Most of the methods fail to remove the color shift for Underwater images, and most of them generally appear bluish. In contrast, the Gray World assumption method removes the bluish tone but suffers from severe red artifacts due to overcompensation of the red channel. To overcome the loss of the red channel in an Underwater image, we introduce a new model to compensate for the red channel in the image at every pixel location. \cite{16} We can express the compensated red channel as \cite{16}:
$${I}_{rc}(x)={I}_{r}(x)+ \alpha . (\bar {I}_{g}-\bar {I}_{r}). (1 - {I}_{r}(x)) . {I}_{g}(x),$$
where $I_{r}, I_{g}$ is the normalized red and green color channel, $\bar {I}_{r},\bar {I}_{g}$ is the mean value of the red and green color channels of the image, and $\alpha$ is the constant parameter which in our case is 1. In harsh Underwater environments, blue light may be attenuated as the organic matter absorbs light. So, to address this case, compensation of the red channel is not sufficient, and we also need to compensate for the blue channel compensation \cite{16}. We compute the blue channel compensation as \cite{16}:
 $${I}_{bc}(x)={I}_{b}(x)+ \alpha . (\bar {I}_{g}-\bar {I}_{b}). (1 - {I}_{b}(x)) . {I}_{g}(x),$$
 where $I_{b}, I_{g}$ represent blue and green color channels, and we set the constant parameter $\alpha$ to be 1. After applying for the red and blue color compensation, we use the Gray-world assumption to help in white-balancing the image \cite{16}.\\
  After white-balancing the image, we introduce two pairs of input images to enhance the color contrast and edge sharpness of the white-balanced image. The white-balanced image suffers significantly as more light is absorbed as we go deep Underwater, as absorbed colors are difficult to be recovered \cite{16}. So, we do a gamma correction of the white balanced image to obtain our first input image. Gamma correction is vital to correct the color contrast in the image and increase the difference between the darker/lighter regions of the image as white-balanced images appear to be too bright \cite{16}. In gamma correction, we use the gamma $\gamma$ and the gain (a constant multiplier) to be 1.\\
  Gamma correction of an image result in loss of details in under/overexposed regions of the image. So, we use the sharpened version of the white balanced image to compensate for the loss of details in gamma-corrected image \cite{16}. To get the sharpened image, we blend the Gaussian filtered version of the image with the image to sharpen the image \cite{16}.  From this principle, the sharpened image is defined as:
  $$S = \left ({ I + \mathcal {N} \left \{{ I - G \ast I }\right \} }\right )/2,$$
  $\mathcal {N}\{.\}$ represents the linear normalization operator/the histogram stretching, $G \ast I$ represents the Gaussian filtered version of the image ($I$) \cite{16}. The histogram stretching function "shifts and scales all pixel intensities with a unique shifting and scaling factor. The set of transformed pixel values cover the entire dynamic range" \cite{16}. This sharpened second input helps in reducing degradation caused by scattering of light.\\
  \\
  For the multi-scale fusion process, for each input we use weights such that while blending, we use weight maps in a way where high weight values are represented more in the final image \cite{16}. We use three different weight maps, namely Laplacian contrast weight ($W_{C}$), Saliency weight ($W_{S}$), and the Saturation weight ($W_{sat}$).\\
  We use the laplacian contrast weight to estimate the global contrast by computing the absolute value of a Laplacian filter on each input luminance channel \cite{16}. Laplacian filter helps in tone mapping and extending the depth of view but fails to distinguish between the ramp and the flat region \cite{16}.\\
   We use the saliency weight to emphasize the salient object that loses its importance in Underwater imaging \cite{16}. To calculate the saliency level, we use the saliency estimator by Achanta et al. \cite{28}, who defines saliency as a euclidean distance between the mean image feature $(I_{\mu})$ and the pixel vector value in the Gaussian blurred version $(I_{\omega_{h{\rm c}}})$. We define the saliency map as \cite{28}:
   $$W_{S} =\Vert {\bf I}_{\mu}-{\bf I}_{\omega_{h{\rm c}}}(x, y)\Vert.$$
   However, the Saliency map tends to favor the regions with high luminance values. To overcome this limitation, we use the saturation weight maps based on the fact that the saturation of the region with high luminance decreases \cite{16}. We compute the weight maps as the deviation between the red, green, and blue color channels and luminance $L$ of the $k^{th}$ input \cite{16}. We can write the Saturation weight as \cite{16}:
   $${W}_{Sat}=\sqrt {\frac{1}{3}\left [{(R_{k}\!-\!L_{k})^{2}\!+\!(G_{k}\!-\!L_{k})^{2}\!+\!(B_{k}\!-\!L_{k})^{2}}\right ]}\quad.$$
   Each of the three weight maps is merged into a single normalized weight map $(\bar W_K)$ by summing up the three weight maps and then dividing the weights of each pixel in each map by the sum of all the weights of the same pixels over all the maps \cite{16}. If we represent the weights as $W_{w1}$ and $W_{w2}$, $w \in \{C, S, SAT\}$ for the two input images, we can compute $\bar W_K$ as \cite{16}:
  $$\bar W_K = \frac{(W_{CK} + W_{SK} + W_{SATK} + \delta)}{\sum_{k=1}^{K} (W_{CK} + W_{SK} + W_{SATK}) + K.\delta},$$
  where $K$ is the input number i.e 2, and $\delta$ is a regularization term which ensure each input contributes to the output \cite{16}. For this method we set the $\delta$ to be $0.1$ \cite{16}.\\
  Finally, after weight computations, we can do the multi-scale fusion process using the Laplacian pyramid. We decompose an image into a sum of bandpass images in a Laplacian pyramid representation. In each pass, we filter the image with a low-pass Gaussian kernel $G$ and decimate the filtered image by a factor of 2 in both directions \cite{16}. We deduct an up-sampled version of the low pass image from the input image, which approximates the Laplacian, and uses the decimated low-pass image as an input for the subsequent pyramid step \cite{16}. Using $G_l$ as a sequence of low-pass Gaussian filtering, which represented the Gaussian pyramid and followed by l up-scaling operations, we can define N levels $L_l$ (Laplacian pyramids) as \cite{16}:
  \begin{align*}
  I(x) &= I(x) - G_1\{I(x)\} + G_1\{I(x)\}\\
  &= L_1\{I(x)\} + G_1\{I(x)\} - G_2\{I(x)\} + G_2\{I(x)\}\\
  &= L_1\{I(x)\} + L_2\{I(x)\} + G_2\{I(x)\} - G_3\{I(x)\} + G_3\{I(x)\}\\
  &= ...\\
  &= ...\\
  &= \sum_{l=1}^{N} L_l\{I(x)\}
  \end{align*}
  Both Gaussian and Laplacian pyramids have the same number of levels, so if we can combine the Gaussian and Laplacian Pyramids at each level independently using the Gaussian weights \cite{16}. With l denoting the pyramid levels and k representing the number of input images, we define  the combined pyramid $R_l$ as \cite{16}:
  $$R_l(x) = \sum_{k=1}^K G_l\{\bar W_k(x)\}L_l\{I_k(x)\}$$
  We can summerize the "Color Balance and Fusion for Underwater Image Enhancement" method in the following algorithm:
  \begin{algorithm}
	\caption*{Color Balance and Fusion for Underwater Image Enhancement \cite{26}}
	\begin{algorithmic}

	\Procedure{Enhance}{degraded\_image}
    \State $I_{rc}$ = Compensate the red channel
    \State $I_{bc}$ = Compensate the blue channel
    \State $I_{wb}$ = White-balancing using Gray-World Assumption
    \State $I_{gamma}$ = Gamma corrected first input image from $I_{wb}$
    \State $I_{sharp}$ = Sharpened second input image from $I_{wb}$
    \State
    \State $W_C$ = Calculate Laplacian Weight for both inputs $I_{gamma} \: I_{sharp}$
    \State $W_S$ = Calculate Saliency Weight for both inputs $I_{gamma} \: I_{sharp}$
    \State $W_{SAT}$ = Calculate Saturation Weight for both inputs $I_{gamma} \: I_{sharp}$
    \State $\bar W$ = Merge Weight maps into a single normalized weight maps
    \State
    \State $G$ = Calculate the Gaussian Pyramid
    \State $L$ = Calculate the Laplacian Pyramid
    \State
    \State $R$ = Mix Laplacian inputs with the Gaussian normalized weights
    \State
    \State $fusion$ = Recover the enhanced RGB image
    
    \Return $fusion$
\EndProcedure

\end{algorithmic}
\end{algorithm}\\
\subsection{An Underwater Color Image Quality Evaluation Metric \cite{18}}
After performing enhancement methods on Underwater Images, we need to compare these output enhanced images among each other to find the best enhancement method for our Underwater images. We can classify quantitative image quality evaluation metrics by whether a reference image representing the original scene exists \cite{18}. Since we do not have the reference image for our Underwater scene, we cannot use the reference-based evaluation metric.\\
To select the best metric for evaluating the Underwater images, we must consider the correlation to the subjective test data and the computation cost. UCIQE uses the CIELab color space, a uniform and device independent color space \cite{18}. So, if $I_p$ is pixel value of an image in CIELab color space, where $p \in [1,N]$, then $I_p = [l_p, a_p, b_p]$ and $c_I$ is the chroma, we can define "Underwater Color Image Quality Evaluation Metric" (UCIQE) \cite{18} as:
$$UCIQE = c_1 \times \sigma_c + c_2 \times con_l + c_3 \times \mu_s.$$
Here, $\sigma_c$ is the standard deviation of the chroma, $con_l$ represents the contrast of the luminance, $\mu_s$ represents the average of the saturation, and $c_1, \: c_2, \: c_3$ are the weighted coefficients \cite{18}. \\
The UCIQE metric extracts the most relevant CIELab space statistical features that represent the image distortion in the Underwater scene, like the color casts, blurring, and noise due to attenuation of light \cite{18}. 
  \section{Evaluation of the Investigation}
  \label{sec:4}

  %This section discusses criteria that are used to evaluate the
  %research results. Make sure your results can be used to published
  %research results, i.e., to the already known state-of-the-art.

  %(target size: 5-10 pages)


\section{Conclusions}
\label{sec:5}

  Summarize the main aspects and results of the research
  project. Provide an answer to the research questions stated earlier.

  (target size: 1/2 page)
  
  \newpage
  
  %\bibliographystyle{unsrt} 
  %\bibliography{bsc-sample}
  \printbibliography
  
\end{document}
